# Docling Cluster Pipeline Implementation Plan

## Goal

Create a deterministic, hash-anchored document processing pipeline using IBM Docling, PyTorch embeddings, and a local-first Docker Compose deployment.

## Proposed Changes

---

### Pipeline Directory Structure

#### [NEW] [pipeline/](file:///c:/Users/eqhsp/.gemini/antigravity/playground/ghost-void/pipeline/)

```
pipeline/
├── docker-compose.yml
├── schemas/
│   ├── doc.normalized.v1.schema.json
│   └── chunk.embedding.v1.schema.json
├── lib/
│   ├── canonical.py       # JCS hash + ledger
│   └── normalize.py       # L2 norm + text policy
├── ingest_api/
│   ├── Dockerfile
│   ├── requirements.txt
│   └── main.py            # FastAPI
├── docling_worker/
│   ├── Dockerfile
│   ├── requirements.txt
│   └── worker.py          # Celery/Arq task
├── embed_worker/
│   ├── Dockerfile
│   ├── requirements.txt
│   └── worker.py          # PyTorch embeddings
└── ledger/
    └── ledger.jsonl       # Append-only hash chain
```

---

### Core Modules

#### [NEW] [lib/canonical.py](file:///c:/Users/eqhsp/.gemini/antigravity/playground/ghost-void/docling-pipeline/lib/canonical.py)

- `jcs_canonical_bytes(obj)` – RFC8785-style JSON canonicalization
- `sha256_hex(b)` – SHA256 hex digest
- `hash_canonical_without_integrity(payload)` – Strips `integrity`, hashes, re-injects
- `append_to_ledger(record, ledger_path)` – Appends with `prev_ledger_hash`

#### [NEW] [lib/normalize.py](file:///c:/Users/eqhsp/.gemini/antigravity/playground/ghost-void/docling-pipeline/lib/normalize.py)

- `normalize_text(text)` – NFKC, collapse whitespace, LF endings
- [l2_normalize(tensor)](file:///c:/Users/eqhsp/.gemini/antigravity/playground/ghost-void/docling-pipeline/services/embed_worker/worker.py#40-43) – PyTorch L2 normalization

---

### Services

#### [NEW] `ingest_api/main.py` (FastAPI)

- `POST /ingest` – Accepts file upload + metadata, returns `bundle_id`
- Enqueues to `parse_queue` (Redis)

#### [MODIFY] [docling_worker/tasks.py](file:///c:/Users/eqhsp/.gemini/antigravity/playground/ghost-void/docling-pipeline/services/docling_worker/tasks.py)
- Consumes `parse_queue`
- Parses with IBM Docling
- Normalizes text
- **Batches chunks** (e.g., groups of 32)
- Enqueues batches to `embed_queue`

#### [MODIFY] [embed_worker/worker.py](file:///c:/Users/eqhsp/.gemini/antigravity/playground/ghost-void/docling-pipeline/services/embed_worker/worker.py)
- Consumes `embed_queue`
- **Processes batches** using PyTorch batch inference
- Uses L2 normalization on batch tensors
- Performs bulk upsert to Qdrant
- Appends multiple records to ledger efficiently

---

### Infrastructure

#### [NEW] [docker-compose.yml](file:///c:/Users/eqhsp/.gemini/antigravity/playground/ghost-void/docling-pipeline/docker-compose.yml)

Services:

- `redis:alpine` – Queue backend
- `qdrant:latest` – Vector store
- `ingest-api` – FastAPI (port 8000)
- `docling-worker` – Celery worker
- `embed-worker` – Celery worker

Volumes:

- `./ledger:/data/ledger` – Persistent ledger

---

## Determinism Anchors (ConfigMap-ready)

| Key | Example |
|-----|---------|
| `pipeline_version` | `v1.0.0` |
| `docling_version` | `0.4.0` |
| `normalizer_version` | `norm.v1` |
| `chunker_version` | `chunk.v1` |
| `embedder_model_id` | `sentence-transformers/all-MiniLM-L6-v2` |
| `weights_hash` | `sha256:abc...` |

---

## Verification Plan

### Replay Test

1. Submit a known document to `/ingest`
2. Capture `doc_id`, `chunk_ids`, embedding hashes
3. Purge and re-process
4. Assert identical hashes

### Ledger Integrity

- Verify each record's `prev_ledger_hash` chains correctly
- Reject records with missing integrity fields
